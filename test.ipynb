{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import train_models, final_models, dataset, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = train_models.TrainModel(final_models.LineRecognition, dataset.OCRDataset, params.params, params.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0) loss: inf\n",
      "Iteration 1 of epoch 0) loss: nan\n",
      "Iteration 2 of epoch 0) loss: nan\n",
      "Iteration 3 of epoch 0) loss: nan\n",
      "Iteration 4 of epoch 0) loss: nan\n",
      "Iteration 5 of epoch 0) loss: nan\n",
      "Iteration 6 of epoch 0) loss: nan\n",
      "Iteration 7 of epoch 0) loss: nan\n",
      "Iteration 8 of epoch 0) loss: nan\n",
      "Iteration 9 of epoch 0) loss: nan\n",
      "Iteration 10 of epoch 0) loss: nan\n",
      "Iteration 11 of epoch 0) loss: nan\n",
      "Iteration 12 of epoch 0) loss: nan\n",
      "Iteration 13 of epoch 0) loss: nan\n",
      "Iteration 14 of epoch 0) loss: nan\n",
      "Iteration 15 of epoch 0) loss: nan\n",
      "Iteration 16 of epoch 0) loss: nan\n",
      "Iteration 17 of epoch 0) loss: nan\n",
      "Iteration 18 of epoch 0) loss: nan\n",
      "Iteration 19 of epoch 0) loss: nan\n",
      "Iteration 20 of epoch 0) loss: nan\n",
      "Iteration 21 of epoch 0) loss: nan\n",
      "Iteration 22 of epoch 0) loss: nan\n",
      "Iteration 23 of epoch 0) loss: nan\n",
      "Iteration 24 of epoch 0) loss: nan\n",
      "Iteration 25 of epoch 0) loss: nan\n",
      "Iteration 26 of epoch 0) loss: nan\n",
      "Iteration 27 of epoch 0) loss: nan\n",
      "Iteration 28 of epoch 0) loss: nan\n",
      "Iteration 29 of epoch 0) loss: nan\n",
      "Iteration 30 of epoch 0) loss: nan\n",
      "Iteration 31 of epoch 0) loss: nan\n",
      "Iteration 32 of epoch 0) loss: nan\n",
      "Iteration 33 of epoch 0) loss: nan\n",
      "Iteration 34 of epoch 0) loss: nan\n",
      "Iteration 35 of epoch 0) loss: nan\n",
      "Iteration 36 of epoch 0) loss: nan\n",
      "Iteration 37 of epoch 0) loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_model.fit(0.0001, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ctc_loss() received an invalid combination of arguments - got (Tensor, list, list, list, int, int, bool), but expected one of:\n * (Tensor log_probs, Tensor targets, tuple of ints input_lengths, tuple of ints target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n * (Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/OCR-ANN-P1/test.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B37.32.26.130/home/ubuntu/OCR-ANN-P1/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m target \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B37.32.26.130/home/ubuntu/OCR-ANN-P1/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m out_length \u001b[39m=\u001b[39m [\u001b[39m2000\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B37.32.26.130/home/ubuntu/OCR-ANN-P1/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m res \u001b[39m=\u001b[39m ctc_loss(output, target, out_length, target_length)\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/modules/loss.py:1744\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mctc_loss(log_probs, targets, input_lengths, target_lengths, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblank, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1745\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_infinity)\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/functional.py:2616\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   2610\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2611\u001b[0m         ctc_loss,\n\u001b[1;32m   2612\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   2613\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   2614\u001b[0m         blank\u001b[39m=\u001b[39mblank, reduction\u001b[39m=\u001b[39mreduction, zero_infinity\u001b[39m=\u001b[39mzero_infinity\n\u001b[1;32m   2615\u001b[0m     )\n\u001b[0;32m-> 2616\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mctc_loss(\n\u001b[1;32m   2617\u001b[0m     log_probs, targets, input_lengths, target_lengths, blank, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), zero_infinity\n\u001b[1;32m   2618\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: ctc_loss() received an invalid combination of arguments - got (Tensor, list, list, list, int, int, bool), but expected one of:\n * (Tensor log_probs, Tensor targets, tuple of ints input_lengths, tuple of ints target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n * (Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n"
     ]
    }
   ],
   "source": [
    "ctc_loss = torch.nn.CTCLoss()\n",
    "target_lengths = [30, 25, 20]\n",
    "input_lengths = [50, 50, 50]\n",
    "targets = torch.randint(1, 15, (sum(target_lengths),), dtype=torch.int)\n",
    "log_probs = torch.randn(50, 3, 15, dtype=torch.float).log_softmax(2)\n",
    "res = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "\n",
    "output = torch.randn(3,200,2000)\n",
    "target_length = [6]\n",
    "target = [\"hello\"]\n",
    "out_length = [2000]\n",
    "\n",
    "res = ctc_loss(output, target, out_length, target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataset import OCRDataset, dataloader_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from modules import params\n",
    "\n",
    "dataset = OCRDataset(params.params)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=dataloader_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(130.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([\n",
    "[[1,2,3], [1,2,3]]\n",
    ",[[4,5,6], [5,6,2]]\n",
    ", [[7,8,9], [7,5,9]]\n",
    ", [[7,8,9], [7,5,9]]\n",
    "], dtype=torch.float32)\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  3.,  5.]],\n",
       "\n",
       "        [[ 7.,  9., 11.]],\n",
       "\n",
       "        [[13., 15., 17.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "b = Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "b(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 20.], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.float32([10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9000e+08])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([990000000], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('OCR-ANN-P1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e65a229dedefb52743e486ffe93c48ec5d4f320d52c129d5e85a7bbb3acbfeb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
