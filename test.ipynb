{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import train_models, final_models\n",
    "model = final_models.LineRecognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = train_models.TrainModel(final_models.LineRecognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ctc_loss() received an invalid combination of arguments - got (Tensor, list, list, list, int, int, bool), but expected one of:\n * (Tensor log_probs, Tensor targets, tuple of ints input_lengths, tuple of ints target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n * (Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/OCR-ANN-P1/test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B37.32.26.130/home/ubuntu/OCR-ANN-P1/test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_model\u001b[39m.\u001b[39;49mfit(\u001b[39m0.0001\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/OCR-ANN-P1/modules/train_models.py:32\u001b[0m, in \u001b[0;36mTrainModel.fit\u001b[0;34m(self, opt_lr, num_epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m     31\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(imgs)\n\u001b[0;32m---> 32\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, gts, [img\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m img \u001b[39min\u001b[39;49;00m imgs], [\u001b[39mlen\u001b[39;49m(gt) \u001b[39mfor\u001b[39;49;00m gt \u001b[39min\u001b[39;49;00m gts])\n\u001b[1;32m     33\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/modules/loss.py:1744\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mctc_loss(log_probs, targets, input_lengths, target_lengths, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblank, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1745\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_infinity)\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/functional.py:2616\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   2610\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2611\u001b[0m         ctc_loss,\n\u001b[1;32m   2612\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   2613\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   2614\u001b[0m         blank\u001b[39m=\u001b[39mblank, reduction\u001b[39m=\u001b[39mreduction, zero_infinity\u001b[39m=\u001b[39mzero_infinity\n\u001b[1;32m   2615\u001b[0m     )\n\u001b[0;32m-> 2616\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mctc_loss(\n\u001b[1;32m   2617\u001b[0m     log_probs, targets, input_lengths, target_lengths, blank, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), zero_infinity\n\u001b[1;32m   2618\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: ctc_loss() received an invalid combination of arguments - got (Tensor, list, list, list, int, int, bool), but expected one of:\n * (Tensor log_probs, Tensor targets, tuple of ints input_lengths, tuple of ints target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n * (Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n"
     ]
    }
   ],
   "source": [
    "train_model.fit(0.0001, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from modules.train_models import OCRDataset\n",
    "from torchvision.transforms import Compose, Resize\n",
    "\n",
    "dataset = OCRDataset(\"create-data/data/\", transforms=Compose([Resize((200, 2000))]))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,2])\n",
    "a.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([\"hello\", \"hi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "S = 30      # Target sequence length of longest target in batch \n",
    "\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss = torch.nn.CTCLoss()\n",
    "target_lengths = [30, 25, 20]\n",
    "input_lengths = [50, 50, 50]\n",
    "targets = torch.randint(1, 15, (sum(target_lengths),), dtype=torch.int)\n",
    "log_probs = torch.randn(50, 3, 15, dtype=torch.float).log_softmax(2)\n",
    "res = ctc_loss(log_probs, targets, input_lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.randn(3,200,2000)\n",
    "target_length = [6]\n",
    "target = [\"hello\"]\n",
    "out_length = [2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ctc_loss() received an invalid combination of arguments - got (Tensor, list, list, list, int, int, bool), but expected one of:\n * (Tensor log_probs, Tensor targets, tuple of ints input_lengths, tuple of ints target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n * (Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/OCR-ANN-P1/test.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B37.32.26.130/home/ubuntu/OCR-ANN-P1/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m res \u001b[39m=\u001b[39m ctc_loss(output, target, out_length, target_length)\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/modules/loss.py:1744\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mctc_loss(log_probs, targets, input_lengths, target_lengths, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblank, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1745\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_infinity)\n",
      "File \u001b[0;32m~/OCR-ANN-P1/lib/python3.10/site-packages/torch/nn/functional.py:2616\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   2610\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2611\u001b[0m         ctc_loss,\n\u001b[1;32m   2612\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   2613\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   2614\u001b[0m         blank\u001b[39m=\u001b[39mblank, reduction\u001b[39m=\u001b[39mreduction, zero_infinity\u001b[39m=\u001b[39mzero_infinity\n\u001b[1;32m   2615\u001b[0m     )\n\u001b[0;32m-> 2616\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mctc_loss(\n\u001b[1;32m   2617\u001b[0m     log_probs, targets, input_lengths, target_lengths, blank, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), zero_infinity\n\u001b[1;32m   2618\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: ctc_loss() received an invalid combination of arguments - got (Tensor, list, list, list, int, int, bool), but expected one of:\n * (Tensor log_probs, Tensor targets, tuple of ints input_lengths, tuple of ints target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n * (Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, int reduction, bool zero_infinity)\n      didn't match because some of the arguments have invalid types: (Tensor, !list!, !list!, !list!, int, int, bool)\n"
     ]
    }
   ],
   "source": [
    "res = ctc_loss(output, target, out_length, target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('OCR-ANN-P1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e65a229dedefb52743e486ffe93c48ec5d4f320d52c129d5e85a7bbb3acbfeb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
